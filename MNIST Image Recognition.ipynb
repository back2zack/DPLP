{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39686d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np#\n",
    "import matplotlib.pyplot as plt\n",
    "import torch \n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74cf8b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.Resize((28,28)),\n",
    "                               transforms.ToTensor(),\n",
    "                               transforms.Normalize((0.5,), (0.5,))\n",
    "                               ])\n",
    "training_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "validation_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3544a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loader = torch.utils.data.DataLoader(training_dataset, batch_size = 100, shuffle = True)\n",
    "validation_loader = torch.utils.data.DataLoader(validation_dataset, batch_size =100 , shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb193d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "type(training_dataset)\n",
    "\n",
    "image, label  = training_dataset[0]\n",
    "print(image.shape)\n",
    "# image  = image.numpy().squeeze()\n",
    "# image = image*0.5 + 0.5\n",
    "# plt.imshow(image, cmap='gray')\n",
    "# plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9617465",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f17a0cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, num_in, H1 ,H2, num_out):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(num_in,H1)\n",
    "        self.layer2 = nn.Linear(H1,H2)\n",
    "        self.layer3 = nn.Linear(H2,num_out)\n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        # i assume the soft max should be applied here !? (becaus the value won't be between 0-1)\n",
    "        x = self.layer3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a11e9fda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of Classifier(\n",
       "  (layer1): Linear(in_features=784, out_features=125, bias=True)\n",
       "  (layer2): Linear(in_features=125, out_features=125, bias=True)\n",
       "  (layer3): Linear(in_features=125, out_features=10, bias=True)\n",
       ")>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Classifier(784, 125, 125, 10)\n",
    "model.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55d9ba85",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67c98020",
   "metadata": {},
   "outputs": [],
   "source": [
    "# When reshaping a tensor, the new shape must contain the same total number \n",
    "#of elements as the old shape. For example, if the original shape was [100, 1, 28, 28], \n",
    "#there are a total of 100 * 1 * 28 * 28 = 78400 elements.\n",
    "# If you want to reshape this tensor to have a shape of [100, something],\n",
    "#then something must be 78400 / 100 = 784 in order for the total number of elements\n",
    "#to remain the same. You could calculate this yourself and use view(100, 784),\n",
    "#but PyTorch allows you to just write view(100, -1), and it will automatically calculate \n",
    "#that the size of the second dimension should be 784."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fef5ffa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------\n",
      "2.071519613265991\n",
      "tensor(57.)\n",
      "-------------\n",
      "4.115206718444824\n",
      "tensor(125.)\n",
      "-------------\n",
      "6.148845911026001\n",
      "tensor(188.)\n",
      "-------------\n",
      "2.0217666625976562\n",
      "tensor(61.)\n",
      "-------------\n",
      "4.059501647949219\n",
      "tensor(111.)\n",
      "-------------\n",
      "6.101106882095337\n",
      "tensor(169.)\n"
     ]
    }
   ],
   "source": [
    "epochs = 2\n",
    "loss_history = []\n",
    "correct_predection_history = []\n",
    "\n",
    "for e  in  range(epochs):\n",
    "    running_loss= 0.0\n",
    "    running_corrects = 0.0\n",
    "    val_running_loss= 0.0\n",
    "    val_running_corrects = 0.0\n",
    "    for batch, counter in zip(training_loader, range(3)):\n",
    "        examples , labels = batch\n",
    "        examples = examples.view(examples.shape[0],-1)\n",
    "        y_dach = model(examples)\n",
    "        loss = criterion(y_dach,labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        _, preds = torch.max(y_dach, 1)\n",
    "        running_loss += loss.item()\n",
    "        running_corrects += torch.sum(preds == labels)\n",
    "    else: \n",
    "        with torch.no_grad():\n",
    "            for val_batch , counter in zip(validation_loader, range(3)):\n",
    "                val_examples , val_labels = val_batch\n",
    "                val_examples = val_examples.view(val_examples.shape[0],-1)\n",
    "                val_y = model(val_examples)\n",
    "                loss = criterion(val_y, val_labels)\n",
    "                _,val_preds = torch.max(val_y, 1)\n",
    "                val_running_loss += loss.item()\n",
    "                val_running_corrects += torch.sum(val_preds == val_labels)\n",
    "                print('-------------')\n",
    "                print(val_running_loss)\n",
    "                print(val_running_corrects)\n",
    "                \n",
    "    \n",
    "                \n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5dd013",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
